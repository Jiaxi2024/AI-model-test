# Feature Specification: 统一多模态模型评测网页

**Feature Branch**: `001-multimodal-model-eval-web`  
**Created**: 2026-02-06  
**Status**: Draft  
**Input**: User description: "团队需要一个统一的模型评测网页，用于快速测试阿里云新模型在多模态（文本/语音/图片/视频）输入下的输出效果，并支持关键词、批量测试与结果留存，便于对比不同模型/参数/提示词的表现。要求用 Python 实现，单脚本启动；支持麦克风实时语音输入；文字输入支持 AI 自动补全；界面清晰美观，含历史记录、模型对比、数据报表等功能模块。"

## Clarifications

### Session 2026-02-06

- Q: 模型输出是流式逐字展示还是等待完整响应后一次性展示？ → A: 流式展示，模型返回内容逐字/逐段实时渲染，用户可即时看到部分输出。
- Q: 历史记录的数据保留策略是什么？ → A: 永久保留 + 手动清理，默认永久存储所有记录，用户可在历史记录页面手动删除单条或批量清理。
- Q: API Key 的配置方式是什么？ → A: 双模式，服务端通过环境变量或配置文件配置默认 Key，用户也可在网页端覆盖为自己的 Key。
- Q: 模型对比组数是否需要支持未来扩展到 2 组以上？ → A: MVP 固定 2 组对比，架构预留扩展能力，未来可支持更多组。
- Q: 模型 API 请求的超时阈值是多少？ → A: 60 秒，超时后展示友好错误信息并提供重试选项。

## User Scenarios & Testing *(mandatory)*

### User Story 1 - 单模型多模态推理测试 (Priority: P1)

评测人员打开网页，选择一个模型（如 Qwen-Omni），在输入区域中输入文本提示词、上传图片/视频文件、或通过麦克风录入语音，然后点击"发送"按钮，系统将多模态内容发送给模型，并在输出区域实时展示模型返回的结果（文本、语音播放等）。

**Why this priority**: 这是平台最核心的功能——让用户能快速用不同模态的输入测试模型的响应能力，是所有后续功能的基础。

**Independent Test**: 可以通过选择一个模型、输入一段文字/上传一张图片/录制一段语音，点击发送后查看输出结果来独立验证。提供了基本的模型测试价值。

**Acceptance Scenarios**:

1. **Given** 用户已进入评测页面，**When** 用户选择模型、输入文本并点击发送，**Then** 系统在输出区域展示模型返回的文本结果。
2. **Given** 用户已进入评测页面，**When** 用户选择模型、上传一张图片并附带文本提示词后点击发送，**Then** 系统将图片与文本一起发送给模型，并展示模型的响应。
3. **Given** 用户已进入评测页面，**When** 用户选择模型、上传一段视频并附带文本提示词后点击发送，**Then** 系统将视频与文本一起发送给模型，并展示模型的响应。
4. **Given** 用户已进入评测页面，**When** 用户点击麦克风按钮后说话，录音完成后点击发送，**Then** 系统将语音数据发送给模型，并展示模型的响应（文本和/或语音播放）。
5. **Given** 用户输入了多种模态内容（如文本+图片+语音），**When** 用户点击发送，**Then** 系统将所有模态内容组合发送给模型，输出区域完整展示模型响应。

---

### User Story 2 - 文字输入 AI 自动补全 (Priority: P2)

评测人员在文本输入框中输入提示词时，系统根据已输入的内容实时提供 AI 自动补全建议。用户可以通过快捷键（如 Tab）接受补全建议，提高输入效率。

**Why this priority**: AI 自动补全直接提升用户输入效率，减少重复输入工作量，是良好用户体验的关键组成部分。

**Independent Test**: 可以在文字输入框输入部分内容，验证是否出现补全建议；按 Tab 或点击建议验证是否自动填充。

**Acceptance Scenarios**:

1. **Given** 用户正在文本输入框中打字，**When** 用户输入了至少若干字符，**Then** 系统在输入框下方或内联位置展示 AI 补全建议。
2. **Given** 补全建议已出现，**When** 用户按 Tab 键或点击建议项，**Then** 建议内容自动填充到输入框中。
3. **Given** 补全建议已出现，**When** 用户继续输入其他字符，**Then** 补全建议实时更新或消失。

---

### User Story 3 - 双模型对比测试 (Priority: P2)

评测人员希望对比两个不同模型（或同一模型不同参数/提示词）在相同输入下的输出差异。用户进入"模型对比"功能页面，选择两组模型/参数配置，输入相同的多模态内容，系统同时调用两组配置并将结果并排展示。

**Why this priority**: 模型对比是评测工作的核心需求之一，能帮助团队快速判断不同模型或参数配置的优劣。

**Independent Test**: 可以通过选择两个不同模型、输入相同的文本，查看并排展示的两组输出来独立验证。

**Acceptance Scenarios**:

1. **Given** 用户进入模型对比页面，**When** 用户分别为左右两组选择不同的模型，输入相同内容后点击发送，**Then** 系统同时调用两个模型并将结果左右并排展示。
2. **Given** 用户进入模型对比页面，**When** 用户为两组选择同一模型但不同参数（如不同 temperature），**Then** 系统使用不同参数调用同一模型，并将两组结果并排展示。
3. **Given** 对比结果已展示，**When** 用户查看结果区域，**Then** 每组结果都清楚标注了所用模型名称、参数配置以及响应耗时。

---

### User Story 4 - 关键词与批量测试 (Priority: P3)

评测人员需要使用关键词进行批量测试。用户输入一组关键词列表，系统依据关键词自动拼接提示词模板并逐一发送给模型，批量获取结果，最终以列表或表格形式展示所有结果。

**Why this priority**: 批量测试能大幅提升评测效率，但依赖于基础的单次推理功能已经完善。

**Independent Test**: 可以通过输入多个关键词、选择提示词模板、执行批量测试后查看结果列表来独立验证。

**Acceptance Scenarios**:

1. **Given** 用户进入批量测试页面，**When** 用户输入多个关键词（逗号或换行分隔）并选择提示词模板后点击执行，**Then** 系统逐一用关键词替换模板中的占位符，依次发送请求，并以表格形式展示每个关键词对应的模型输出。
2. **Given** 批量测试正在执行中，**When** 用户查看页面，**Then** 系统展示当前进度（已完成/总数）和实时结果更新。
3. **Given** 批量测试完成，**When** 用户查看结果，**Then** 用户可以对结果进行筛选、排序，并可导出为文件。

---

### User Story 5 - 历史记录查看与管理 (Priority: P3)

评测人员希望回顾之前的测试记录。用户进入"历史记录"页面，可以查看所有过往的测试请求和响应，支持按时间、模型、模态类型等条件筛选和搜索。

**Why this priority**: 结果留存和回顾是评测工作的重要支撑，便于长期跟踪模型表现变化。

**Independent Test**: 可以在执行若干测试后进入历史记录页面，验证是否能看到完整的测试记录并进行筛选。

**Acceptance Scenarios**:

1. **Given** 用户已进行过多次测试，**When** 用户进入历史记录页面，**Then** 系统按时间倒序展示所有历史测试记录，包含输入内容摘要、模型名称、时间戳和输出摘要。
2. **Given** 用户在历史记录页面，**When** 用户按模型名称筛选或输入关键字搜索，**Then** 列表实时过滤显示匹配的记录。
3. **Given** 用户点击某条历史记录，**When** 详情展开或跳转，**Then** 用户可以看到该次测试的完整输入（含附件预览）和完整输出。

---

### User Story 6 - 数据报表与用量统计 (Priority: P3)

评测人员或管理者希望了解整体使用情况。用户进入"数据报表"页面，可以查看已消耗的总 Token 数、按模型/时间段的使用分布、测试次数统计等可视化报表。

**Why this priority**: 数据报表帮助团队管理资源消耗和评估使用效率，但属于辅助分析功能，优先级低于核心测试功能。

**Independent Test**: 可以在多次测试后进入数据报表页面，验证 Token 消耗统计、测试次数图表是否正确展示。

**Acceptance Scenarios**:

1. **Given** 系统已积累一定量的测试数据，**When** 用户进入数据报表页面，**Then** 系统展示已消耗的总 Token 数、总测试次数等核心指标。
2. **Given** 用户在数据报表页面，**When** 用户选择按模型或按时间段查看，**Then** 系统以图表形式展示相应的使用分布数据。
3. **Given** 用户查看报表，**When** 用户将鼠标悬停在图表元素上，**Then** 系统展示该数据点的详细信息（如具体数值、时间、模型名称）。

---

### Edge Cases

- 上传的文件格式不受模型支持时，系统应如何提示用户？（应在发送前校验文件格式并给出清晰的错误提示）
- 上传文件超过大小限制时，系统应如何处理？（应在上传时校验文件大小，超限时提示用户压缩或更换文件）
- 麦克风权限被浏览器拒绝时，系统应如何引导用户？（应展示明确的权限申请引导和手动上传音频的替代方案）
- 模型 API 请求超时或返回错误时，系统应如何展示？（应在输出区域展示友好的错误信息，包含重试按钮）
- 批量测试中途某个关键词请求失败时，系统应如何处理？（应标记该条为失败，继续执行剩余任务，最终汇总失败项）
- 网络断开时，用户的输入内容是否会丢失？（应在本地缓存用户输入，网络恢复后可重新提交）
- 同时进行多组对比测试或批量测试时，系统的并发能力如何保障？（应通过任务队列管理并发请求，避免相互阻塞）

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: 系统必须提供模型选择功能，用户可以从可用模型列表中选择目标模型（初始支持阿里云 Qwen 系列，包括 Qwen-Omni 全模态模型）。
- **FR-002**: 系统必须支持文本输入，用户可以在输入区域中编写提示词/指令文本。
- **FR-003**: 系统必须支持图片上传，用户可以上传图片文件作为模型输入（支持常见图片格式如 JPG、PNG、GIF、WebP）。
- **FR-004**: 系统必须支持视频上传，用户可以上传视频文件作为模型输入（支持常见视频格式如 MP4、WebM）。
- **FR-005**: 系统必须支持语音输入，包括音频文件上传和浏览器端实时麦克风录音两种方式。
- **FR-006**: 系统必须支持多模态混合输入，用户可以在单次请求中同时包含文本、图片、视频、语音等多种模态内容。
- **FR-007**: 系统必须在输出区域以流式方式实时展示模型响应结果（文本逐字/逐段渲染），同时支持语音播放。
- **FR-008**: 系统必须提供文字输入 AI 自动补全功能，在用户输入提示词时实时给出智能补全建议。
- **FR-009**: 系统必须支持双模型对比功能（MVP 固定 2 组，架构预留未来扩展到更多组的能力），用户可以选择两组不同的模型/参数配置，使用相同输入同时请求并将结果并排展示。
- **FR-010**: 系统必须支持关键词批量测试功能，用户输入关键词列表和提示词模板后，系统自动逐一拼接并发送请求，结果以表格形式展示。
- **FR-011**: 系统必须自动保存所有测试请求和响应记录并永久保留，支持历史记录查看、筛选、搜索，以及手动删除单条记录和批量清理。
- **FR-012**: 系统必须提供数据报表页面，展示 Token 消耗统计、测试次数统计、按模型/时间段的使用分布等可视化图表。
- **FR-013**: 系统必须通过单次运行一个 Python 脚本即可启动完整的前后端服务。
- **FR-014**: 系统必须在上传文件前校验文件格式和大小，不符合要求时给出清晰的错误提示。
- **FR-015**: 系统必须在模型请求失败或超时（超时阈值为 60 秒）时，向用户展示友好的错误信息并提供重试选项。
- **FR-016**: 系统必须在批量测试过程中展示实时进度，并在单条失败时继续执行剩余任务。
- **FR-017**: 界面必须采用 Google Material Design 3 设计语言，企业级淡色风格，具备清晰的分栏布局（左侧 Navigation Rail 导航 + 右侧内容区），包含输入区、输出区、功能导航（推理测试、模型对比、批量测试、历史记录、数据报表、设置等模块入口）。

### Key Entities

- **模型配置 (Model Configuration)**: 代表一个可用的模型实例，包含模型名称、版本、API 端点等属性，以及可调参数（如 temperature、max_tokens 等）。
- **测试记录 (Test Record)**: 代表一次完整的评测交互，包含输入内容（文本、文件引用）、所选模型配置、输出结果、Token 消耗数、响应耗时、时间戳等信息。
- **关键词批次 (Keyword Batch)**: 代表一次批量测试任务，包含关键词列表、提示词模板、关联的模型配置，以及该批次下所有测试记录的汇总。
- **对比会话 (Comparison Session)**: 代表一次模型对比任务，包含两组模型配置、共用的输入内容，以及两组对应的输出结果和性能指标。
- **用量统计 (Usage Statistics)**: 按模型、按时间段聚合的 Token 消耗数据和测试次数数据，用于报表展示。

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: 用户从打开页面到完成首次单模型文本推理测试，整个流程耗时不超过 1 分钟。
- **SC-002**: 用户上传图片/视频/音频文件后，系统在 3 秒内完成文件处理并准备好发送（不含模型推理时间）。
- **SC-003**: 麦克风录音功能在主流浏览器（Chrome、Edge、Firefox）上均可正常使用，录音开始延迟不超过 1 秒。
- **SC-004**: AI 自动补全建议在用户停止输入后 500 毫秒内出现。
- **SC-005**: 双模型对比页面能同时展示两组结果，用户无需滚动即可在同一视窗内看到两组输出的核心内容。
- **SC-006**: 批量测试支持单次处理至少 50 个关键词，所有结果在完成后可一键导出。
- **SC-007**: 历史记录页面加载 1000 条记录的筛选和搜索结果耗时不超过 2 秒。
- **SC-008**: 数据报表页面正确展示 Token 总消耗数和测试次数，图表可按模型和时间段切换查看。
- **SC-009**: 整个系统通过执行单个 Python 脚本即可完成启动，启动后 10 秒内可在浏览器中访问。
- **SC-010**: 90% 的首次使用者能在无额外指导的情况下完成一次完整的模型测试流程（文本输入 → 查看输出）。

## Assumptions

- 系统支持双模式 API Key 配置：服务端通过环境变量或配置文件配置默认 API Key 供团队共享使用，用户也可在网页端设置自己的 Key 进行覆盖。
- 用户使用的浏览器为现代主流浏览器（Chrome、Edge、Firefox 最近两个主要版本），且支持 Web Audio API（麦克风录音所需）。
- 初期仅支持阿里云模型服务 API，但架构设计应便于后续扩展到其他厂商的模型服务。
- 文件上传大小限制遵循阿里云模型 API 的要求（图片不超过 10MB，视频不超过 100MB，音频不超过 25MB）。
- 系统为团队内部使用工具，用户数量有限（预计不超过 20 人同时使用），暂不需要复杂的用户认证和权限管理。
- 数据存储使用本地数据库或文件存储方案，暂不需要云端持久化。
- AI 自动补全功能调用与主评测功能相同的模型 API 服务。
